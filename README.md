# llm_vision_service

the LLM use qwen2_vl 2B model

the service is for talk_with_llm agent call

https://github.com/dibaotian/talk_with_llm_web_version

It can be deployed with the talk_with_llm in same of different server
